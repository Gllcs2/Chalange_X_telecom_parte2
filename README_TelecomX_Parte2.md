# ğŸ“Š PrediÃ§Ã£o de Rotatividade de Clientes (Churn)

## ğŸš€ VisÃ£o Geral
Este projeto implementa um pipeline de **Machine Learning** para prever clientes com alta probabilidade de cancelar um serviÃ§o (rotatividade).  
O objetivo Ã© fornecer uma ferramenta de apoio para estratÃ©gias de retenÃ§Ã£o, baseada em dados histÃ³ricos.

O processo envolve:
- AnÃ¡lise exploratÃ³ria inicial.
- PrÃ©-processamento de dados numÃ©ricos e categÃ³ricos.
- Balanceamento de classes com **SMOTE**.
- Treinamento de modelos **Random Forest** e **Gradient Boosting**.
- AvaliaÃ§Ã£o detalhada dos resultados e anÃ¡lise de variÃ¡veis mais relevantes.

---

## ğŸ”§ Tecnologias Utilizadas
- **Python** (Pandas, NumPy, Matplotlib, Seaborn, Scikit-learn, Imbalanced-learn)
- **Jupyter Notebook** ou **Google Colab**
- **Machine Learning** (modelos ensemble)
- **SMOTE** para balanceamento de classes

---

## ğŸ“‚ Estrutura do Projeto
```bash
â”œâ”€â”€ dados_df.csv                   # Dataset principal
â”œâ”€â”€ TelecomX_Parte2.ipynb          # Notebook com anÃ¡lise completa
â”œâ”€â”€ README.md                      # DocumentaÃ§Ã£o do projeto
```

---

## ğŸ”„ Fluxo de Trabalho
1. **Carregamento e inspeÃ§Ã£o inicial**  
   VisualizaÃ§Ã£o das primeiras linhas, tipos de dados e estatÃ­sticas bÃ¡sicas.

2. **AnÃ¡lise exploratÃ³ria**  
   - IdentificaÃ§Ã£o de valores ausentes.  
   - DistribuiÃ§Ã£o da variÃ¡vel alvo (Rotatividade).

3. **PrÃ©-processamento**  
   - NormalizaÃ§Ã£o de variÃ¡veis numÃ©ricas.
   - One-Hot Encoding para variÃ¡veis categÃ³ricas.
   - Balanceamento com SMOTE no conjunto de treino.

4. **Modelagem**  
   - Treinamento de Random Forest e Gradient Boosting.

5. **AvaliaÃ§Ã£o**  
   - RelatÃ³rios de classificaÃ§Ã£o.  
   - Matrizes de confusÃ£o.  
   - Curvas ROC comparativas.

6. **ImportÃ¢ncia das variÃ¡veis**  
   - Top 15 variÃ¡veis mais influentes para cada modelo.

7. **ConclusÃµes**  
   - InterpretaÃ§Ã£o dos resultados e prÃ³ximos passos.

---

## ğŸ“ˆ Principais Resultados
| Modelo           | AUC Score |
|------------------|-----------|
| Random Forest    | ~0.83     |
| Gradient Boosting| ~0.84     |

- O **Gradient Boosting** teve leve vantagem em sensibilidade para prever clientes de risco.
- O **SMOTE** melhorou o equilÃ­brio das classes no treinamento.
- Algumas variÃ¡veis se destacaram como fortes preditoras, como `Charges.Total` e `EstÃ¡gio`.

---

## ğŸš€ PrÃ³ximos Passos
- Testar outros algoritmos como XGBoost ou LightGBM.
- Criar dashboard interativo para visualizaÃ§Ã£o em tempo real.
- Automatizar a atualizaÃ§Ã£o do modelo.

---

## ğŸ‘¨â€ğŸ’» Autor
**Gabriel Lucas Maia**
